{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Transforming the PDF files into vector database  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial Setup\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import os\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "import chromadb\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ[\"MATHPIX_API_ID\"] = os.getenv('MATHPIX_API_KEY')\n",
    "#openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Langchain framework\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "## The following loaders are used for options\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import MathpixPDFLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35630ca8-a707-4445-b8a2-661fe3312d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial variable setup\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "db_directory = \"./data/chroma_semantic\"\n",
    "USE_Anthropic = True\n",
    "\n",
    "if USE_Anthropic:\n",
    "    llm = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\", temperature=0)\n",
    "else:\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # context window size 16k for GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF Files and Convert to a Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and extract text from PDFs in a folder\n",
    "def get_file_name(source_path):\n",
    "    return source_path.split('/')[-1]\n",
    "\n",
    "def load_pdfs_from_folder(folder_path, loader_option = 5):\n",
    "    # Get a list of PDF files in the specified folder\n",
    "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_name = get_file_name(pdf_file)\n",
    "        \n",
    "        if loader_option == 1:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file)\n",
    "        elif loader_option == 2:\n",
    "            # PyPDFium2Loader is known to be faster than PyPDFLoader\n",
    "            loader = PyPDFium2Loader(pdf_file)\n",
    "        elif loader_option == 3:\n",
    "            # PyMuPDFLoader is known to be general purpose, rich metadata\n",
    "            loader = PyMuPDFLoader(pdf_file)\n",
    "        elif loader_option == 4:\n",
    "            # Allows automated concatenate pages\n",
    "            loader = PDFMinerLoader(pdf_file, concatenate_pages=True)\n",
    "        elif loader_option == 5:\n",
    "            # Use Mathpix OCR to load formula, tables\n",
    "            # may be slower, but higher quality than all above\n",
    "            # Require Mathpix API ID - 3 cents per pdf page\n",
    "            loader = MathpixPDFLoader(pdf_file)\n",
    "        \n",
    "        loaded_docs = loader.load()\n",
    "        \n",
    "        for doc in loaded_docs:\n",
    "            doc.metadata['source'] = file_name\n",
    "        \n",
    "        docs.extend(loaded_docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b938f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Original Chunk\n",
    "############################################################################\n",
    "collection_list=[\n",
    "    \"ASOP_life\",\n",
    "    \"Bermuda\",\n",
    "    \"CFT\",\n",
    "    \"VM21\",\n",
    "    \"VM22\",\n",
    "    \"Asset\",\n",
    "    \"IFRS17\"\n",
    "]\n",
    "\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/'+collection_name\n",
    "\n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_pdfs_from_folder(folder_path, loader_option = 5)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=1000, # 1000 splits a page into roughly 3 chunks\n",
    "    #     chunk_overlap=200,\n",
    "    #     length_function=len,)\n",
    "\n",
    "    # Split the documents into chunks using the text splitter\n",
    "    #splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Use semantic chunker to increase meaningfulness of the chunks\n",
    "    text_splitter = SemanticChunker(embeddings_model)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create a Chroma vector database from the document splits\n",
    "    Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embeddings_model, \n",
    "        persist_directory=db_directory,\n",
    "        collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. Retrieve from the Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a user may choose different collection name from the list\n",
    "# [\"ASOP_life\", \"Bermuda\", \"CFT\", \"VM21\", \"VM22\", \"Asset\", \"IFRS17\"]\n",
    "collection_name = collection_list[0] \n",
    "\n",
    "# Get a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=embeddings_model, \n",
    "                     persist_directory=db_directory,\n",
    "                     collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "# Create a retriever using the vector database as the search source\n",
    "# You may choose a specific document to filter the search\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={\n",
    "                                        'k': 6, \n",
    "                                        'lambda_mult': 0.5,\n",
    "                                        # 'filter': {'source': '201611-Guidance-Notes-for-Commercial-Insurers-and-Groups-Statutory-Reporting-Regime-30-Nov-2016.pdf'}\n",
    "                                        }\n",
    "                                    ) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "qa_system_prompt = \"\"\"You are a helpful assistant to help actuaries with question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "ASOP or asop means Actuarial Standards of Practice. \\\n",
    "CFT means Cash Flow Testing. AAT means Asset Adequacy Testing. \\\n",
    "BMA means Bermuda Monetary Authority. \\\n",
    "SBA means scenario-based approach. BEL means best estimate liabilities.\\\n",
    "After you answer, provide the sources you used to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "\n",
    "{context}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    #sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "# 4. Generate Q&A Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            #markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            markdown_output += \"- **Source {}**: {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "# Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36183436",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "- https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All\n",
    "- https://chat.langchain.com/\n",
    "- https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628913a4",
   "metadata": {},
   "source": [
    "# Management of the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=db_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e514b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete_collection(name=\"CFT\") # Delete a collection and all associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdeddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(name=\"CFT\") \n",
    "collection.count()\n",
    "collection.peek()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ValAct_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
