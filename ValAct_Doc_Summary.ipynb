{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary chain\n",
    "## 1. Set up environment\n",
    "Install necessary packages and set up environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment variables\n",
    "import dotenv\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import modules\n",
    "Import modules required for summary chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# For semantic chunking - not used\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# For laod summarize chain - not used\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup model\n",
    "Setup LLM model and prompts for stuff chain (summary of a full document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "llm = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\", temperature=0)\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a summary of the following document using the format provided. The summary includes the title, publisher and published date if available, purpose and scope, key points, conclusion and implications, and key words. The summary should be comprehensive yet brief, aiming for a reading time of no more than one minute. Avoid any translation or substitution of actuarial terms in the document. When starting a summary, begin the summary with \"Title:\" without saying \"Here is a summary\". Here is the summary format:\n",
    "\n",
    "Title: [title of the document]\\n\\nPublisher and Published Date: [published date and publisher's name]\\n\\nPurpose and Scope: [note the purpose and scope]\\n\\nKey Points:\\n- [indicate key points in bullet point format]\\n\\nConclusions and Implications: [describe conclusion and implications for regulatory and practical purposes in the actuarial field]\\n\\nKey words: [indicate top five key words from the document]\n",
    "\n",
    "Here is the document to summarize:\n",
    "\n",
    "\"{text}\"\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load summarize chain\n",
    "### 4.1 For one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the key points from the actuarial document on valuation requirements for variable annuities:\n",
       "\n",
       "Title: Valuation Manual - VM-21: Requirements for Principle-Based Reserves for Variable Annuities\n",
       "\n",
       "Purpose and Scope: This section establishes the minimum reserve valuation standards for variable annuity contracts with guarantees. It constitutes the Commissioners Annuity Reserve Valuation Method (CARVM) for these contracts.\n",
       "\n",
       "Key Points:\n",
       "- Requires a stochastic reserve calculation on a combined group of contracts using prudent estimate assumptions and stochastic scenarios for equity returns and interest rates.\n",
       "- The stochastic reserve is calculated as the Conditional Tail Expectation (CTE) 70 of the projected accumulated deficiencies under a set of capital market scenarios.\n",
       "- Provides requirements for mapping funds to prescribed asset categories and fund data sources.\n",
       "- Details methodologies for determining prudent estimate mortality, lapse, premium persistence, and other behavioral assumptions.\n",
       "- Outlines requirements for the additional standard projection amount using either the Conditional Tail Expectation with Prescribed Assumptions (CTEPA) method or the Compnay-Specific Market Path (CSMP) method.\n",
       "- Allows an alternative methodology for contracts with no guaranteed living benefits.\n",
       "- Specifies guidance for modeling revenue sharing, hedging programs, and reinsurance.\n",
       "- Requires an allocation of the aggregate reserve to individual contracts based on risk analysis.\n",
       "\n",
       "Conclusions and implications: This prescribed reserve methodology aims to achieve a consistent valuation standard across companies for variable annuity guarantees by defining specific requirements around stochastic projections, prudent estimate assumptions, valuation scenarios, modeling techniques, and reporting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf_path = \"./data/pdf/VM21/202401 VM-21 pbr_data_valuation_manual_future_edition.pdf\"\n",
    "loader = PDFMinerLoader(pdf_path, concatenate_pages=True)\n",
    "doc = loader.load()\n",
    "\n",
    "output = stuff_chain.invoke(doc)\n",
    "\n",
    "display(Markdown(output['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loop through all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# PDF File summarizer\n",
    "#########################################################################\n",
    "# Define the collection list\n",
    "collection_list = [\n",
    "    # \"AI_BigData\",\n",
    "    # \"ASOP_life\", #?? to update\n",
    "    # \"Bermuda\", # done\n",
    "    # \"CFT\", # done\n",
    "    # \"PBR\", # done\n",
    "    # \"VM21\", # done\n",
    "    # \"VM22\", # done\n",
    "    # \"Asset\", # done\n",
    "    # \"IFRS17\" # done\n",
    "]\n",
    "\n",
    "# Check if the JSON file exists\n",
    "if os.path.exists(\"summary.json\"):\n",
    "    # Load existing data from the file\n",
    "    with open(\"summary.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    # Initialize an empty list if the file doesn't exist\n",
    "    results = {}\n",
    "\n",
    "# Loop through each collection\n",
    "for collection_name in collection_list:\n",
    "    # Define the directory path for the current collection\n",
    "    collection_dir = f\"./data/upload/pdf/{collection_name}\"\n",
    "\n",
    "    # Loop through each PDF file in the collection directory\n",
    "    for file_name in os.listdir(collection_dir):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            # Construct the PDF file path\n",
    "            pdf_path = os.path.join(collection_dir, file_name)\n",
    "\n",
    "            # Load the PDF using PDFMinerLoader\n",
    "            loader = PDFMinerLoader(pdf_path, concatenate_pages=True)\n",
    "            doc = loader.load()\n",
    "\n",
    "            # Invoke the summarization chain\n",
    "            output = stuff_chain.invoke(doc)\n",
    "\n",
    "            # Create a dictionary for the current record\n",
    "            record = {\n",
    "                \"collection_name\": collection_name,\n",
    "                \"summary\": output['output_text']\n",
    "            }\n",
    "\n",
    "            # Add the record to the results dictionary using the file name as the key\n",
    "            results[file_name] = record\n",
    "\n",
    "# Save the results as a JSON file\n",
    "with open(\"summary.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Markdown File summarizer\n",
    "#########################################################################\n",
    "# Define the collection list\n",
    "collection_list=[\n",
    "    # \"Cayman\",\n",
    "    # \"AI_BigData\",\n",
    "    # \"ASOP_life\",\n",
    "    # \"Bermuda\",\n",
    "    # \"CFT\",\n",
    "    # \"GAAP\",\n",
    "    # \"RiskFinance\",\n",
    "    # \"PBR\",\n",
    "    # \"VM21\",\n",
    "    # \"VM22\",\n",
    "    # \"Asset\",\n",
    "    \"IFRS17\",\n",
    "    \"SAP\",\n",
    "]\n",
    "\n",
    "# Check if the JSON file exists\n",
    "if os.path.exists(\"summary.json\"):\n",
    "    # Load existing data from the file\n",
    "    with open(\"summary.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    # Initialize an empty list if the file doesn't exist\n",
    "    results = {}\n",
    "\n",
    "# Loop through each collection\n",
    "for collection_name in collection_list:\n",
    "    # Define the directory path for the current collection\n",
    "    collection_dir = f\"./data/upload/md/{collection_name}\"\n",
    "\n",
    "    # Loop through each markdown file in the collection directory\n",
    "    for file_name in os.listdir(collection_dir):\n",
    "        if file_name.endswith(\".md\"):\n",
    "            # Construct the markdown file path\n",
    "            md_path = os.path.join(collection_dir, file_name)\n",
    "\n",
    "            # Load the markdown file using UnstructuredMarkdownLoader\n",
    "            loader = UnstructuredMarkdownLoader(md_path)\n",
    "            doc = loader.load()\n",
    "\n",
    "            # Invoke the summarization chain\n",
    "            output = stuff_chain.invoke(doc)\n",
    "\n",
    "            # Create a dictionary for the current record\n",
    "            record = {\n",
    "                \"collection_name\": collection_name,\n",
    "                \"summary\": output['output_text']\n",
    "            }\n",
    "            file_name_pdf = file_name.replace(\".md\", \".pdf\")\n",
    "            # Add the record to the results dictionary using the file name as the key\n",
    "            results[file_name_pdf] = record\n",
    "\n",
    "# Save the results as a JSON file\n",
    "with open(\"summary.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
