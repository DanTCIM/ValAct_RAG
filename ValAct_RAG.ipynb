{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Converting the PDF files into vector database  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial setup\n",
    "## 1.1. Imports\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import chromadb\n",
    "import json\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ[\"MATHPIX_API_ID\"] = os.getenv('MATHPIX_API_KEY')\n",
    "#openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Langchain framework\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "## The following loaders are used for options\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import MathpixPDFLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84652248",
   "metadata": {},
   "source": [
    "## 1.2. Initial variable setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35630ca8-a707-4445-b8a2-661fe3312d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial variable setup\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "db_directory = \"./data/chroma_semantic\"\n",
    "USE_Anthropic = True\n",
    "\n",
    "if USE_Anthropic:\n",
    "    llm = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\", temperature=0)\n",
    "else:\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # context window size 16k for GPT 3.5 Turbo\n",
    "\n",
    "collection_list=[\n",
    "    \"ASOP_life\",\n",
    "    \"Bermuda\",\n",
    "    \"CFT\",\n",
    "    \"PBR\",\n",
    "    \"VM21\",\n",
    "    \"VM22\",\n",
    "    \"Asset\",\n",
    "    \"IFRS17\"\n",
    "]\n",
    "#collection_list = [\"PBR\"] # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF files and convert to a vector DB\n",
    "## 2.1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and extract text from PDFs in a folder\n",
    "def get_file_name(source_path):\n",
    "    return source_path.split('/')[-1]\n",
    "\n",
    "def load_pdfs_from_folder(folder_path, loader_option):\n",
    "    # Get a list of PDF files in the specified folder\n",
    "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_name = get_file_name(pdf_file)\n",
    "        \n",
    "        if loader_option == 1:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file)\n",
    "        elif loader_option == 2:\n",
    "            # PyPDFium2Loader is known to be faster than PyPDFLoader\n",
    "            loader = PyPDFium2Loader(pdf_file)\n",
    "        elif loader_option == 3:\n",
    "            # PyMuPDFLoader is known to be general purpose, rich metadata\n",
    "            loader = PyMuPDFLoader(pdf_file)\n",
    "        elif loader_option == 4:\n",
    "            # Allows automated concatenate pages\n",
    "            loader = PDFMinerLoader(pdf_file, concatenate_pages=True)\n",
    "        \n",
    "        loaded_docs = loader.load()\n",
    "        \n",
    "        for doc in loaded_docs:\n",
    "            doc.metadata['source'] = file_name\n",
    "        \n",
    "        docs.extend(loaded_docs)\n",
    "    return docs\n",
    "\n",
    "def pdf_to_md(folder_path, download_path, loader_option):\n",
    "    # Get a list of PDF files in the specified folder\n",
    "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    for pdf_file in pdf_files:\n",
    "        file_name = get_file_name(pdf_file)\n",
    "        base_name = file_name.replace('.pdf', '')\n",
    "        \n",
    "        if loader_option == 1:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file)\n",
    "        elif loader_option == 2:\n",
    "            # PyPDFium2Loader is known to be faster than PyPDFLoader\n",
    "            loader = PyPDFium2Loader(pdf_file)\n",
    "        elif loader_option == 3:\n",
    "            # PyMuPDFLoader is known to be general purpose, rich metadata\n",
    "            loader = PyMuPDFLoader(pdf_file)\n",
    "        elif loader_option == 4:\n",
    "            # Allows automated concatenate pages\n",
    "            loader = PDFMinerLoader(pdf_file, concatenate_pages=True)\n",
    "        elif loader_option == 5:\n",
    "            # Use Mathpix OCR to load formula, tables\n",
    "            # may be slower, but higher quality than all above\n",
    "            # Require Mathpix API ID - 3 cents per pdf page\n",
    "            loader = MathpixPDFLoader(pdf_file)\n",
    "        \n",
    "        loaded_docs = loader.load()\n",
    "        \n",
    "        for i, doc in enumerate(loaded_docs):\n",
    "            doc.metadata['source'] = file_name\n",
    "            if loader_option > 3:\n",
    "                md_file_name = f\"{download_path}/{base_name}.md\"\n",
    "            else:\n",
    "                md_file_name = f\"{download_path}/{base_name}{i+1:03d}.md\"\n",
    "            with open(md_file_name, 'w', encoding='utf-8') as md_file:\n",
    "                md_file.write(doc.page_content)\n",
    "\n",
    "def load_mds_from_folder(folder_path):\n",
    "    # Get a list of md files in the specified folder\n",
    "    md_files = glob.glob(f\"{folder_path}/*.md\")\n",
    "    docs = []\n",
    "    for md_file in md_files:\n",
    "        file_name = get_file_name(md_file)\n",
    "        base_name = file_name.replace('.md', '')\n",
    "        pdf_file_name = f\"{base_name}.pdf\"\n",
    "        \n",
    "        loader = UnstructuredMarkdownLoader(md_file)\n",
    "        loaded_docs = loader.load()\n",
    "        \n",
    "        for doc in loaded_docs:\n",
    "            doc.metadata['source'] = pdf_file_name\n",
    "        \n",
    "        docs.extend(loaded_docs)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ad53a",
   "metadata": {},
   "source": [
    "## 2.2. Convert PDFs to markdown files and then convert to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb282c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run only to convert pdf to markdown files\n",
    "############################################################################\n",
    "\n",
    "collection_list = [\"AI_BigData\"]\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/pdf/'+collection_name\n",
    "    download_path = './data/upload/md/'+collection_name\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "    # Use loader option 5 to use Mathpix OCR to load formula, tables\n",
    "    pdf_to_md(folder_path, download_path, loader_option = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run to load markdown files to vector database\n",
    "############################################################################\n",
    "collection_list=[\n",
    "    \"AI_BigData\",\n",
    "    \"ASOP_life\",\n",
    "    # \"Bermuda\",\n",
    "    \"CFT\",\n",
    "    # \"PBR\",\n",
    "    # \"VM21\",\n",
    "    # \"VM22\",\n",
    "    # \"Asset\",\n",
    "    # \"IFRS17\",\n",
    "]\n",
    "\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/md/'+collection_name\n",
    "\n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_mds_from_folder(folder_path)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=1000, # 1000 splits a page into roughly 3 chunks\n",
    "    #     chunk_overlap=200,\n",
    "    #     length_function=len,)\n",
    "\n",
    "    # Use semantic chunker to increase meaningfulness of the chunks\n",
    "    text_splitter = SemanticChunker(embeddings_model)\n",
    "\n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create a Chroma vector database from the document splits\n",
    "    Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embeddings_model, \n",
    "        persist_directory=db_directory,\n",
    "        collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b27f5",
   "metadata": {},
   "source": [
    "## 2.3. (NOT USED) Convert PDFs to vector database directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run to load pdf files to vector database\n",
    "############################################################################\n",
    "\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/pdf/'+collection_name\n",
    "\n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_pdfs_from_folder(folder_path, loader_option = 1)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=1000, # 1000 splits a page into roughly 3 chunks\n",
    "    #     chunk_overlap=200,\n",
    "    #     length_function=len,)\n",
    "\n",
    "    # Use semantic chunker to increase meaningfulness of the chunks\n",
    "    text_splitter = SemanticChunker(embeddings_model)\n",
    "\n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create a Chroma vector database from the document splits\n",
    "    Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embeddings_model, \n",
    "        persist_directory=db_directory,\n",
    "        collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. For test purposes\n",
    "## 3.1. Define vector store from vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a user may choose different collection name from the list\n",
    "# [\"ASOP_life\", \"Bermuda\", \"CFT\", \"VM21\", \"VM22\", \"Asset\", \"IFRS17\"]\n",
    "collection_name = collection_list[0] \n",
    "\n",
    "# Get a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=embeddings_model, \n",
    "                     persist_directory=db_directory,\n",
    "                     collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4d0e8",
   "metadata": {},
   "source": [
    "## 3.2. Retrieve from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "# Create a retriever using the vector database as the search source\n",
    "# You may choose a specific document to filter the search\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={\n",
    "                                        'k': 6, \n",
    "                                        'lambda_mult': 0.5,\n",
    "                                        # 'filter': {'source': '201611-Guidance-Notes-for-Commercial-Insurers-and-Groups-Statutory-Reporting-Regime-30-Nov-2016.pdf'}\n",
    "                                        }\n",
    "                                    ) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "qa_system_prompt = \"\"\"You are a helpful assistant to help actuaries with question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "ASOP or asop means Actuarial Standards of Practice. \\\n",
    "CFT means Cash Flow Testing. AAT means Asset Adequacy Testing. \\\n",
    "BMA means Bermuda Monetary Authority. \\\n",
    "SBA means scenario-based approach. BEL means best estimate liabilities.\\\n",
    "After you answer, provide the sources you used to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "\n",
    "{context}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    #sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "## 3.3. Generate Q&A Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            #markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            markdown_output += \"- **Source {}**: {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36183436",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 4. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "- https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All\n",
    "- https://chat.langchain.com/\n",
    "- https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628913a4",
   "metadata": {},
   "source": [
    "# 5. Management of the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=db_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e514b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete_collection(name=\"VM20\") # Delete a collection and all associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdeddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(name=\"Bermuda\") \n",
    "collection.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5025ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.modify(name=\"PBR\") rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    where={\"source\": \"risk-brief-bermuda-reinsurance_0.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.delete(\n",
    "    #ids=[\"id1\", \"id2\", \"id3\",...],\n",
    "    where={\"source\": \"VA_PN_Supplement_Exposure_Draft.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a6a9c",
   "metadata": {},
   "source": [
    "# Rename process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1410cdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m ids_to_update \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mid\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, metadata \u001b[38;5;129;01min\u001b[39;00m collection\u001b[38;5;241m.\u001b[39mget(where\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: old_file_name})\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m      9\u001b[0m new_metadatas \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_file_name} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m ids_to_update]\n\u001b[0;32m---> 10\u001b[0m collection\u001b[38;5;241m.\u001b[39mupdate(ids\u001b[38;5;241m=\u001b[39mids_to_update, metadatas\u001b[38;5;241m=\u001b[39mnew_metadatas)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# json file summary update\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/RAG_Test/lib/python3.12/site-packages/chromadb/api/models/Collection.py:442\u001b[0m, in \u001b[0;36mCollection.update\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, ids, embeddings, metadatas, documents, uris)\n",
      "File \u001b[0;32m~/anaconda3/envs/RAG_Test/lib/python3.12/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/RAG_Test/lib/python3.12/site-packages/chromadb/api/segment.py:422\u001b[0m, in \u001b[0;36mSegmentAPI._update\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_embedding_record(coll, r)\n\u001b[1;32m    421\u001b[0m     records_to_submit\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_producer\u001b[38;5;241m.\u001b[39msubmit_embeddings(coll[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m], records_to_submit)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_product_telemetry_client\u001b[38;5;241m.\u001b[39mcapture(\n\u001b[1;32m    425\u001b[0m     CollectionUpdateEvent(\n\u001b[1;32m    426\u001b[0m         collection_uuid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RAG_Test/lib/python3.12/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/RAG_Test/lib/python3.12/site-packages/chromadb/db/mixins/embeddings_queue.py:172\u001b[0m, in \u001b[0;36mSqlEmbeddingsQueue.submit_embeddings\u001b[0;34m(self, topic_name, embeddings)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# The returning clause does not guarantee order, so we need to do reorder\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# the results. https://www.sqlite.org/lang_returning.html\u001b[39;00m\n\u001b[1;32m    171\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RETURNING seq_id, id\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Pypika doesn't support RETURNING\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m results \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mexecute(sql, params)\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Reorder the results\u001b[39;00m\n\u001b[1;32m    174\u001b[0m seq_ids \u001b[38;5;241m=\u001b[39m [cast(SeqId, \u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    175\u001b[0m     results\n\u001b[1;32m    176\u001b[0m )  \u001b[38;5;66;03m# Lie to mypy: https://stackoverflow.com/questions/76694215/python-type-casting-when-preallocating-list\u001b[39;00m\n",
      "\u001b[0;31mOperationalError\u001b[0m: attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "# Rename process\n",
    "folder_name = \"Bermuda\"\n",
    "old_file_name = \"risk-brief-bermuda-reinsurance_0.pdf\"\n",
    "new_file_name = \"2024_AAA_risk-brief-bermuda-reinsurance.pdf\"\n",
    "\n",
    "# Chroma DB update\n",
    "collection = client.get_collection(name=folder_name) \n",
    "ids_to_update = [id for id, metadata in collection.get(where={\"source\": old_file_name}).items()]\n",
    "new_metadatas = [{\"source\": new_file_name} for _ in ids_to_update]\n",
    "collection.update(ids=ids_to_update, metadatas=new_metadatas)\n",
    "\n",
    "# json file summary update\n",
    "with open('summary.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "new_key = new_file_name\n",
    "new_data = {new_key: data.pop(old_file_name)}\n",
    "\n",
    "with open('summary.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=4)\n",
    "\n",
    "# md and pdf file update\n",
    "# Construct the paths to the folders\n",
    "md_folder = os.path.join(\"data/md\", folder_name)\n",
    "pdf_folder = os.path.join(\"data/pdf\", folder_name)\n",
    "\n",
    "# Construct the old and new file paths\n",
    "old_file_name_md = old_file_name.replace(\".pdf\", \".md\")\n",
    "new_file_name_md = new_file_name.replace(\".pdf\", \".md\")\n",
    "old_md_path = os.path.join(md_folder, old_file_name_md)\n",
    "new_md_path = os.path.join(md_folder, new_file_name_md)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(old_md_path, new_md_path)\n",
    "\n",
    "# Construct the old and new file paths\n",
    "old_pdf_path = os.path.join(pdf_folder, old_file_name)\n",
    "new_pdf_path = os.path.join(pdf_folder, new_file_name)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(old_pdf_path, new_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92954856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ValAct_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
