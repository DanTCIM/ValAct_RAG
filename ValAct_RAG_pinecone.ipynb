{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Converting the PDF files into vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ[\"MATHPIX_API_ID\"] = os.getenv('MATHPIX_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "from common.utils import (\n",
    "    pdf_to_md,\n",
    "    split_mds,\n",
    ")\n",
    "\n",
    "from common.summarizer import summarize_collections\n",
    "\n",
    "# Langchain framework\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "## Initial variable setup\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "USE_Anthropic = True\n",
    "\n",
    "if USE_Anthropic:\n",
    "    llm = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\", temperature=0)\n",
    "else:\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # context window size 16k for GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF files and convert to a vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ad53a",
   "metadata": {},
   "source": [
    "## 2.1a Convert PDFs to markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb282c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: loaded, waiting for processing to complete\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Run only to convert pdf to markdown files\n",
    "############################################################################\n",
    "\n",
    "collection_list=[\n",
    "    #\"Cayman\",\n",
    "    # \"AI_BigData\",\n",
    "    # \"ASOP_life\",\n",
    "    # \"Bermuda\",\n",
    "    # \"CFT\",\n",
    "    # \"GAAP\",\n",
    "    # \"RiskFinance\",\n",
    "    \"PBR\",\n",
    "    # \"VM21\",\n",
    "    # \"VM22\",\n",
    "    # \"Asset\",\n",
    "    # \"IFRS17\",\n",
    "    # \"SAP\",\n",
    "]\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/pdf/'+collection_name\n",
    "    download_path = './data/upload/md/'+collection_name\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "    # Use loader option 5 to use Mathpix OCR to load formula, tables\n",
    "    pdf_to_md(folder_path, download_path, loader_option = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c447db3",
   "metadata": {},
   "source": [
    "## 2.1b Image formatter in Markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ed2b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: ./data/upload/md/PBR/2024-NAIC-GOES-field-test-instructions.md\n",
      "Processed: ./data/upload/md/PBR/2024-NAIC-VM22 Field Test Specs.md\n"
     ]
    }
   ],
   "source": [
    "def process_img_script(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    def replace_image_link(match):\n",
    "        url = match.group(1)\n",
    "        return f'<img src=\"{url}\" alt=\"image\" style=\"width:100%;height:auto;\">'\n",
    "\n",
    "    modified_content = re.sub(r\"!\\[\\]\\((.*?)\\)\", replace_image_link, content)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(modified_content)\n",
    "\n",
    "\n",
    "def process_img_mds(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                process_img_script(file_path)\n",
    "                print(f\"Processed: {file_path}\")\n",
    "\n",
    "# Specify the folder path containing the .md files\n",
    "#folder_path = './data/md/test'\n",
    "for collection_name in collection_list:\n",
    "    folder_path = './data/upload/md/'+collection_name\n",
    "    process_img_mds(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6668b",
   "metadata": {},
   "source": [
    "## 2.1c Summarize the markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e929fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_collections(collection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1a99b",
   "metadata": {},
   "source": [
    "## 2.2 Set up pincone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249d8d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_pinecone.vectorstores.PineconeVectorStore"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"valact-rag\"\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index = index,\n",
    "    embedding=embeddings_model,\n",
    ")\n",
    "type(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce916cb5",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7872c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain contractual service margin\"  \n",
    "vectorstore.similarity_search(  \n",
    "    query,  # our search query  \n",
    "    k=1,  # return 3 most relevant docs \n",
    "    namespace=\"IFRS17\", \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "query = \"Explain contractual service margin\"  \n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index = index,\n",
    "    embedding=embeddings_model,\n",
    "    namespace=\"IFRS17\",\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "qa.invoke(query) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08bbd1",
   "metadata": {},
   "source": [
    "## 2.3a Loading markdown files to vector database (use Header Text Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bf3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run to load markdown files to vector database\n",
    "############################################################################\n",
    "# collection_list=[\n",
    "#     # \"Cayman\",\n",
    "#     # \"AI_BigData\",\n",
    "#     # \"ASOP_life\",\n",
    "#     \"Bermuda\",\n",
    "#     # \"CFT\",\n",
    "#     # \"GAAP\", # use this\n",
    "#     # \"RiskFinance\",\n",
    "#     # \"PBR\",\n",
    "#     # \"Asset\",\n",
    "#     # \"IFRS17\",\n",
    "#     # \"Product\",\n",
    "#     # \"SAP\"\n",
    "# ]\n",
    "\n",
    "for collection_name in collection_list:\n",
    "    folder_path = './data/upload/md/'+collection_name\n",
    "\n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    splits = split_mds(\n",
    "        folder_path, \n",
    "        IsSemantic=True,\n",
    "        breakpoint_threshold_type_input=\"standard_deviation\",\n",
    "        breakpoint_threshold_amount_input=2,\n",
    "        embeddings_model=embeddings_model,\n",
    "    )\n",
    "    # BREAKPOINT_DEFAULTS: Dict[BreakpointThresholdType, float] = {\n",
    "    #     \"percentile\": 95,\n",
    "    #     \"standard_deviation\": 3,\n",
    "    #     \"interquartile\": 1.5,\n",
    "    # }\n",
    "\n",
    "    # Create a vector database from the document splits\n",
    "    vectorstore.add_documents(\n",
    "        documents=splits,\n",
    "        namespace=collection_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf1e7f",
   "metadata": {},
   "source": [
    "## 2.3b Update document_list.json file to be used in the Streamlit Selector\n",
    "You have to move the pdf files under the pdf folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d973e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, file_path):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "def scan_directory(base_path):\n",
    "    folders_files = {}\n",
    "    for folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            files = [\"All\"]\n",
    "            for file in os.listdir(folder_path):\n",
    "                # Exclude system files like .DS_Store\n",
    "                if file != \".DS_Store\":\n",
    "                    files.append(file)\n",
    "            files[1:] = sorted(files[1:])\n",
    "            folders_files[folder] = files\n",
    "    return folders_files\n",
    "base_path = \"./data/pdf\"\n",
    "\n",
    "document_list = scan_directory(base_path)\n",
    "\n",
    "save_json(document_list, \"./data/document_list.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0889ce5",
   "metadata": {},
   "source": [
    "## 2.3c json file consistency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1faec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking collection: SAP\n",
      "Checking collection: ASOP_life\n",
      "Checking collection: Asset\n",
      "Checking collection: Bermuda\n",
      "Checking collection: AI_BigData\n",
      "Checking collection: CFT\n",
      "Checking collection: Product\n",
      "Checking collection: Cayman\n",
      "Checking collection: RiskFinance\n",
      "Checking collection: IFRS17\n",
      "Checking collection: GAAP\n",
      "Checking collection: PBR\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the link.json file\n",
    "with open(\"./data/document_link.json\") as link_file:\n",
    "    link_data = json.load(link_file)\n",
    "\n",
    "# Read the list.json file\n",
    "with open('./data/document_list.json') as list_file:\n",
    "    list_data = json.load(list_file)\n",
    "\n",
    "# Loop through each collection in the list.json file\n",
    "for collection, items in list_data.items():\n",
    "    print(f\"Checking collection: {collection}\")\n",
    "    \n",
    "    # Loop through each item in the collection\n",
    "    for item in items:\n",
    "        # Check if the item exists as a key in the link.json file\n",
    "        if item not in link_data:\n",
    "            print(f\"  Item not found: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c125b4",
   "metadata": {},
   "source": [
    "## 2.3d Move MD files under the MD folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03b050",
   "metadata": {},
   "source": [
    "## 2.4 Pinecone Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c93a9dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 3072,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'AI_BigData': {'vector_count': 780},\n",
       "                'ASOP_life': {'vector_count': 904},\n",
       "                'Asset': {'vector_count': 408},\n",
       "                'Bermuda': {'vector_count': 1056},\n",
       "                'CFT': {'vector_count': 501},\n",
       "                'Cayman': {'vector_count': 190},\n",
       "                'GAAP': {'vector_count': 831},\n",
       "                'IFRS17': {'vector_count': 161},\n",
       "                'PBR': {'vector_count': 1311},\n",
       "                'Product': {'vector_count': 1463},\n",
       "                'RiskFinance': {'vector_count': 1530},\n",
       "                'SAP': {'vector_count': 267}},\n",
       " 'total_vector_count': 9402}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d0da6",
   "metadata": {},
   "source": [
    "## 2.5 Pinecone Namespace Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_list=[\n",
    "    # \"Cayman\",\n",
    "    # \"AI_BigData\",\n",
    "    # \"ASOP_life\",\n",
    "    # \"Bermuda\",\n",
    "    # \"CFT\",\n",
    "    # \"GAAP\",\n",
    "    # \"RiskFinance\",\n",
    "    # \"PBR\",\n",
    "    # \"VM21\",\n",
    "    # \"VM22\",\n",
    "    # \"Asset\",\n",
    "    # \"IFRS17\",\n",
    "    # \"Product\",\n",
    "]\n",
    "\n",
    "for collection_name in collection_list:\n",
    "    index.delete(delete_all=True, namespace=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. For test purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4d0e8",
   "metadata": {},
   "source": [
    "## 3.1. Retrieve from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "# Create a retriever using the vector database as the search source\n",
    "# You may choose a specific document to filter the search\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={\n",
    "                                        'k': 6, \n",
    "                                        'lambda_mult': 0.5,\n",
    "                                        # 'filter': {'source': '201611-Guidance-Notes-for-Commercial-Insurers-and-Groups-Statutory-Reporting-Regime-30-Nov-2016.pdf'}\n",
    "                                        }\n",
    "                                    ) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "qa_system_prompt = \"\"\"You are a helpful assistant to help actuaries with question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "ASOP or asop means Actuarial Standards of Practice. \\\n",
    "CFT means Cash Flow Testing. AAT means Asset Adequacy Testing. \\\n",
    "BMA means Bermuda Monetary Authority. \\\n",
    "SBA means scenario-based approach. BEL means best estimate liabilities.\\\n",
    "After you answer, provide the sources you used to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "\n",
    "{context}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    #sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "## 3.3. Generate Q&A Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            #markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            markdown_output += \"- **Source {}**: {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36183436",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628913a4",
   "metadata": {},
   "source": [
    "# 4. Management of the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e514b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all documents from the namespace\n",
    "index.delete(delete_all=True, namespace='IFRS17')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69705c",
   "metadata": {},
   "source": [
    "## 4.1a Remove documents from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3835ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filter = {\"source\": \"2020-NAIC-AG48.pdf\"}\n",
    "namespace_name = 'PBR'\n",
    "\n",
    "# Query the index with the metadata filter\n",
    "results = index.query(\n",
    "    vector=[0.0]*3072, \n",
    "    top_k=10000, \n",
    "    include_metadata=True, \n",
    "    filter=metadata_filter,\n",
    "    namespace=namespace_name,\n",
    ")\n",
    "ids = [result.id for result in results['matches']]\n",
    "for id in ids:\n",
    "    print(id)\n",
    "index.delete(ids=ids, namespace=namespace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cda5f",
   "metadata": {},
   "source": [
    "## 4.1b Rename a namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filter = {\"source\": \"202307-NAIC-VM-22 Subgroup Draft.pdf\"}\n",
    "namespace_name = 'VM22'\n",
    "new_namespace_name = 'PBR'\n",
    "\n",
    "# Query the index with the metadata filter\n",
    "results = index.query(\n",
    "    vector=[0.0]*3072, \n",
    "    top_k=10000, \n",
    "    include_metadata=True, \n",
    "    filter=metadata_filter,\n",
    "    namespace=namespace_name,\n",
    ")\n",
    "\n",
    "ids = [result.id for result in results['matches']]\n",
    "fetch_data = index.fetch(ids=ids, namespace=namespace_name)\n",
    "for id in ids:\n",
    "    print(id)\n",
    "    values = fetch_data.vectors[id].values\n",
    "    metadata = fetch_data.vectors[id].metadata\n",
    "    index.upsert(\n",
    "        vectors = [\n",
    "            {\n",
    "                \"id\": id,\n",
    "                \"values\": values,\n",
    "                \"metadata\": metadata,\n",
    "            }\n",
    "        ],\n",
    "        namespace=new_namespace_name\n",
    "    )\n",
    "index.delete(ids=ids, namespace=namespace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f71297",
   "metadata": {},
   "source": [
    "## 4.1c Update part of info - rename metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_name = \"2023-12-20-11-08-13-2023-Year-End-Long-Term-Instructions-Handbook.pdf\"\n",
    "new_name = \"2023-BMA-2023YE-Long-Term-Instructions-Handbook.pdf\"\n",
    "\n",
    "metadata_filter = {\"source\": old_name}\n",
    "namespace_name = 'Bermuda'\n",
    "\n",
    "# Query the index with the metadata filter\n",
    "results = index.query(\n",
    "    vector=[0.0]*3072, \n",
    "    top_k=10000, \n",
    "    include_metadata=True, \n",
    "    filter=metadata_filter,\n",
    "    namespace=namespace_name,\n",
    ")\n",
    "\n",
    "ids = [result.id for result in results['matches']]\n",
    "fetch_data = index.fetch(ids=ids, namespace=namespace_name)\n",
    "for id in ids:\n",
    "    print(id)\n",
    "    index.update(\n",
    "        id=id, \n",
    "        set_metadata={\"source\": new_name}, \n",
    "        namespace=namespace_name\n",
    "    )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d462724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index.fetch(ids=[ids[0]], namespace=namespace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc846031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json file summary update\n",
    "with open('summary.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Update the existing data with the new key-value pair\n",
    "data[new_name] = data.pop(old_name)\n",
    "\n",
    "with open('summary.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002869e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# md and pdf file update\n",
    "# Construct the paths to the folders\n",
    "md_folder = os.path.join(\"data/md\", namespace_name)\n",
    "pdf_folder = os.path.join(\"data/pdf\", namespace_name)\n",
    "\n",
    "# Construct the old and new file paths\n",
    "old_file_name_md = old_name.replace(\".pdf\", \".md\")\n",
    "new_file_name_md = new_name.replace(\".pdf\", \".md\")\n",
    "old_md_path = os.path.join(md_folder, old_file_name_md)\n",
    "new_md_path = os.path.join(md_folder, new_file_name_md)\n",
    "\n",
    "# Check if the old MD file exists before renaming\n",
    "if os.path.exists(old_md_path):\n",
    "    # Rename the MD file\n",
    "    os.rename(old_md_path, new_md_path)\n",
    "else:\n",
    "    print(f\"Skipping MD file rename: {old_md_path} does not exist.\")\n",
    "\n",
    "# Construct the old and new file paths\n",
    "old_pdf_path = os.path.join(pdf_folder, old_name)\n",
    "new_pdf_path = os.path.join(pdf_folder, new_name)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(old_pdf_path, new_pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354529f9",
   "metadata": {},
   "source": [
    "## 4.2 Print IDs for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filter = {\"source\": \"asop014_082_whenToCFT.pdf\"}\n",
    "namespace_name = 'IFRS17'\n",
    "\n",
    "# Query the index with the metadata filter\n",
    "results = index.query(\n",
    "    vector=[0.0]*3072, \n",
    "    top_k=10000, \n",
    "    include_metadata=True, \n",
    "    filter=metadata_filter,\n",
    "    namespace=namespace_name,\n",
    ")\n",
    "\n",
    "ids = [result.id for result in results['matches']]\n",
    "for id in ids:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74103e1e",
   "metadata": {},
   "source": [
    "## 4.3 Get file names from a namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3369bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct sources: ['2016-SOA-Economic-Capital-report.pdf', '2018-AAA-Life_Reins_Reserve_Credit_Practice_Note.pdf', '2022-SOA-RW_vs_RN_scenarios.pdf', '2016-AAA-ORSA_Risk_Exposures_PracticeNote.pdf', '2023-SOA-rating-agency-perspectives.pdf', '2020-AAA-Tax_Cuts_Jobs_ACT_White_Paper.pdf', '2011-SOA-Model_efficiency_study.pdf', '2023-SOA-SP_Global-Capital-Model-Change.pdf', '2024-AAA-liquidity-risk_practice_note.pdf', '2016-SOA-ESG_guide.pdf', '2009-SOA-EmbeddedValue_practice_theory.pdf', '2016-SOA-nested-stochastic-modeling-report.pdf', '2023-AAA-issue-brief-dividend-limitation.pdf', '2024-AAA-issue-brief-group-capital.pdf', '2014-SOACIA-ModelValidation_RIsk_Capital_Models.pdf', '2019-AAA-ModelRiskManagementPracticeNote.pdf', '2010-SOA-OperationalRiskPaper.pdf', '2019-IAIS-ICS-V2-Level-1-Doc.pdf', '2023-SOA-interest-rate-model-calibration.pdf', '2023-SOA-reg-capital-comparison.pdf']\n"
     ]
    }
   ],
   "source": [
    "namespace_name = 'RiskFinance'\n",
    "response = index.query(\n",
    "    vector=[0.0]*3072,\n",
    "    top_k=10000,\n",
    "    include_metadata=True,\n",
    "    namespace=namespace_name,\n",
    ")\n",
    "\n",
    "# Extract the distinct \"source\" names from the response\n",
    "sources = [result[\"metadata\"][\"source\"] for result in response[\"matches\"]]\n",
    "\n",
    "distinct_sources = list(set(sources))\n",
    "print(\"Distinct sources:\", distinct_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60081f",
   "metadata": {},
   "source": [
    "# 5. Tests - to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd988ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list to store the text lengths for each split\n",
    "text_lengths = []\n",
    "split_lentgh = len(splits)\n",
    "print(split_lentgh)\n",
    "for i, doc in enumerate(splits):\n",
    "    text_length = len(doc.page_content)\n",
    "    text_lengths.append(text_length)\n",
    "    if(text_length>40960):\n",
    "        print(doc.metadata['source'])\n",
    "\n",
    "# Create a histogram of text lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(text_lengths, bins=20, edgecolor='black')\n",
    "plt.xlabel('Text Length (bytes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Text Lengths')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd68491",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = \"./data/md/GAAP/2005-AAA-DIGB36_PracticNote.md\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, \n",
    "    strip_headers=False,\n",
    ")\n",
    "loader = TextLoader(md_file)\n",
    "loaded_docs = loader.load()\n",
    "# MD splits\n",
    "md_header_splits = markdown_splitter.split_text(loaded_docs[0].page_content)\n",
    "len(md_header_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5ad02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ValAct_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
