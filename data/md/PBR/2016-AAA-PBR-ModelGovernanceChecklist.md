# Model Governance Checklist 

## Some Considerations for Practicing Life Actuaries

Principle-Based Reserves Checklist Subgroup

August 2016

Michael Failor, MAAA, ASA, Chairperson

Mary Bahna-Nolan, MAAA, FSA, CERA

Michael Boerner, MAAA, ASA

Brian Fomby, MAAA, FSA

Cande Olsen, MAAA, FSA

Peter Weber, MAAA, ASA

American Academy of ActUaries

Ali Zaker-Shahrak, MAAA, FSA

ACTUARY.ORG

The Principle-Based Reserves Checklist Subgroup of the Model Governance Work Group of the American Academy of Actuaries has developed a model governance checklist. This nonexhaustive checklist is offered as a resource for practicing life actuaries involved in actuarial model governance. Its development was prompted in response to the need for good model governance as addressed in PBR regulation; however, it will also be of value wherever actuarial modeling is performed. It should also be made clear that although this checklist represents model governance practices that many life actuaries and their companies would aspire to, they will not apply in every situation, as model governance is a complex and evolving process specific to a company's modeling environment.

A model is defined in the most recent exposure draft of the Modeling Actuarial Standard as "A representation of relationships among variables, entities, or events using statistical, financial, economic, mathematical, or scientific concepts and equations." It is likely that all the models that life actuaries work with will fall into these categories, but for the purposes of this checklist, there is no intention to exclude any model that life actuaries would work with.

While checklist items are presented in question format, they are only intended to foster awareness of the respective model governance concerns. These questions are not intended to be directed toward any specific individual, actuary, or group, but are offered solely as considerations for practicing life actuaries. Furthermore, the checklist is not intended as an instrument for rating or assessing an organization's level of governance; nor is it prescriptive in any way, and does not constitute a list of model governance requirements. There is no expectation that "yes" will always be the right answer to any of the questions posed. The answer will depend on many things, including the nature of the company, the model purpose, and the materiality of the model inputs and outputs. All questions are meant to be considerations and not a judgment that the actuary is doing a good or bad job. There may be a good reason for a "no" answer. For example, A6 asks, "Is there a training program in place to ensure users/modelers are adequately educated and capable of utilizing the modeling software?" The answer may be "no" for a small company that doesn't warrant a formal training program.

It should be further understood that many of the questions contained in this document are best comprehended by life actuaries who are involved in modeling and modeling systems management. Non-actuaries, and those lacking systems management expertise, may be ill-equipped to discern the proper context in which many of the questions apply. Consequently, any interpretation or implication derived from the application or extension of the questions contained in this list would necessitate an appropriate level of relevant experience, knowledge, and professional judgment. This would include an expectation of an understanding of terms that are commonly used in activities related to modeling. There are many terms used in this checklist that are not defined herein, because it is expected that life actuaries involved in modeling would be familiar with these terms.

The Model Governance practice note is an additional source of information for practicing life actuaries seeking to better understand models, model risks, model governance and related issues, as these actuaries implement PBR in their organizations.

# Model Governance Checklist Categories 

The checklist is divided into the following 10 categories:

A Governance Standards<br>B Modeling Process<br>C Assumptions Setting<br>D Input Data/Tables/Mapping<br>E Access Controls<br>F System/Model Changes<br>G Model Selection/Versioning<br>H Consolidation of Results<br>I Reporting<br>J Analysis/Validation

Note that "Governance Standards" and "Modeling Process" categories contain only those questions that are unique to these topics, and therefore do not appear under other categories. Questions touching on any of the remaining eight categories may be duplicated across those categories in which the question is associated.

## Color Coding

The questions are further categorized with the following color codes:

## Governance Policy/Process-Type Questions Validation-Type Questions

The governance policy/process-type questions are high level, as they typically pertain to model governance policies, procedures, and modeling practices. In contrast, the validationtype questions may entail a more hands-on assessment.

## General Guiding vs. Technical Focus Questions

Questions are further categorized under "General Guiding" vs. "Technical Focus." These two categories were difficult to assign, but this somewhat indeterminate classification is intended to aid in assessing the level of required technical knowledge associated with the respective model governance concern. With this in mind, the General Guiding questions may be more likely suited for those involved in model governance management, whereas the Technical Focus questions may be more fully comprehended by those directly involved in day-to-day modeling activities.

## Reference Documents

The following documents served as primary sources in the creation of this checklist:

"A Survey of Actuarial Modeling Controls in the Context of a Model-Based Valuation Framework" Copyright $[2012$ The Society of Actuaries, Schaumburg, Illinois.

"System Access and Change Controls" (The Modeling Platform) @ Dec. 2015 The Society of Actuaries, Schaumburg, Illinois.

"Model Validation for Insurance Enterprise Risk and Capital Models" ( () 2014 Casualty Actuarial Society, Canadian Institute of Actuaries, Society of Actuaries, All Rights Reserved).

"Report from the Actuarial Processes and Controls Best Practices Working Party" (Institute and Faculty of Actuaries, May 2009) - This report was prepared by and/or on behalf of the IFoA. The IFoA does not accept any responsibility and/or liability whatsoever for the content or use of this document. (C) Institute and Faculty of Actuaries.

## Governance Standards

Color Code:

Governance Policy/Process-Type Questions

Validation-Type Questions

## General Guiding Questions

Has a model governance framework been formally developed, documented, and adopted throughout the organization?

Is senior management aware of and familiar with the governance policies?

Is there a dedicated modeling "organizational structure" having clear and delineated responsibilities for development, maintenance, execution, and change management of actuarial systems and models? Such responsibilities include:

- Inventory of models subject to governance
- Creation, maintenance, and communication of all change requests
- Management of approval process
- Integration of changes
- Coordination of testing

Have model documentation standards been established and adopted throughout the organization?

Are modeling documentation standards periodically reviewed for adequacy and utility?

Is there a training program in place to ensure users/modelers are adequately educated and capable of utilizing the modeling software?

Is there a model/system governance committee?

When discrepancies in the data, modeling, systems, or reporting are discovered, is there a process in place to document these issues and to mitigate them?

Are discovered modeling or system errors documented and archived along with their effects? (Archived error/bug documentation is a valuable resource when conducting future reconciliations and testing.) Are processes in place to correct, prevent and/or mitigate such errors?

Are the system and modeling peer reviewers experienced in the systems, models, products, and assets in which they review?

Are there separate test and development environments?

Are modeling procedures and protocols themselves archived when they are updated or modified?

Are modeling/system "change request procedures" widely distributed and well understood throughout the organization?

Is there a satisfactory level of buy-in among the affected departments regarding the modeling procedures and protocols?

## Questions of a More Technical Focus

A15 Is versioning software in use for data, models, and systems?

A16 Has the modeling environment been consolidated away from silos and toward a more holistic environment comprised of fewer platforms and fewer redundant models? If not feasible, have additional controls been implemented to ensure model integrity across all modeling platforms?

A17 Are the impacts of coding modifications/enhancements/corrections captured and documented?

A18 Are model governance protocols strictly enforced during quarterly and annual reporting? (Bypassing protocols may indicate a lack of management buy-in, and/or the need to better address fast-track changes.)

## B

## Modeling Process

Color Code: $\quad$ Governance Policy/Process-Type Questions

Validation-Type Questions

## General Guiding Questions

B1 Does the model inventory include a ranking of models according to significance, risk, financial impact, materiality, etc.? (Such ranking is often used to prioritize ongoing model review/validation.)

B2 Is there a process in place to determine which risks require modeling?

B3 Is there a process to determine which modeling applications are subject to model governance?

B4 Is there a process to assure proper documentation is created according to existing documentation requirements?

B5 Is there a formal model/code checkout system in place?

B6 Is there a formal model/system change request procedure?

B7 Are the intended users/audience of the model output well-defined?

B8 Is there a single source or repository that documents the data flow/system processing from end-to-end?

## Questions of a More Technical Focus

B9 When vendor models are used, are the documented modeling/conceptual limitations vendor-supplied, or user-derived?

B10 Are model outputs reproducible? (i.e., have the inputs and model versions been archived, and has an operable version of the modeling software been retained?)

B11 Does documentation of applied modeling concepts make reference to external sources (either professional, academic, or regulatory sources)?

B12 Does documentation of applied modeling concepts describe how modeling components are connected, along with explanations of why they can be used together?

B13 Are the modeling concepts' limitations and inherent biases adequately emphasized in the documentation?

<img src="https://cdn.mathpix.com/cropped/2024_03_24_c6ed40200ab25ed70b4dg-07.jpg?height=149&width=138&top_left_y=1172&top_left_x=251" alt="image" style="width:100%;height:auto;">

## Assumptions Setting <br> Color Code:

Governance Policy/Process-Type Questions<br>Validation-Type Questions

## General Guiding Questions

Are there regular external reviews of assumption setting methodologies?

Are assumptions (including both deterministic and stochastic) given an independent review?

When setting assumptions, is the "level of analysis" reviewed by a senior staff member to ensure such "level of analysis" is neither too broad nor too narrow for the assessment and intended application of the assumption? (same as J6)

Has an assumption governance framework been formally developed, documented, and adopted throughout the organization? Does it include a centralized approach for common assumption setting and formal approval? Are differences in assumptions across modeling applications documented and approved?

Is there a formal change management process in place for changing assumptions?

Is there a formal process for updating assumptions that have routine calendar-based elements (e.g., economic scenarios, U.S. Treasury curves, etc.), where such assumptions are automatically updated and no judgement is required?

Are assumptions archived and maintained in a known, centralized database/repository? Is write-access to this repository controlled/limited?

Are experience studies performed on a regular schedule, and assumption updates performed at a consistent time each year?

| C9 | Are the components of an assumption defined and documented, including the raw <br> experience data, margin, asset spreads, and projected changes to the margins (e.g., <br> mortality improvement)? |
| :---: | :---: |
| C10 | Does the assumption documentation address the independence of each assumption? If <br> stochastic or deterministic modeling is being performed, does the documentation address <br> the interaction of risks (e.g., interest rates and policyholder behavior) |
| C11 | Is an automated process used to input or feed assumptions into the model(s)? |
| C12 | Prior to implementing a change in an assumption, is an expectation formed and <br> documented of the impact of the change? (Model results can then be compared to these <br> expectations.) |
| C13 | Has the organization adopted and documented a standard process or philosophy for <br> isolating and evaluating the impact of each assumption change? |
| C14 | Do those who are developing assumptions have the necessary skills and experience? |
| C15 | Are all data sources well defined and documented? |
| C16 | Have all input assumptions and parameters been peer reviewed? |
| C17 | Have modeling assumptions for future investment, disinvestment, and management <br> strategies received prior approval? |
| 18 | Have expense assumptions received prior approval? |

## Questions of a More Technical Focus

C19 Are assumptions ranked or prioritized based on materiality/significance/impact/

Are the impacts of dynamic assumptions tested under different economic scenarios (e.g., dynamic lapse assumptions)? (same as J24)

C21 Is there a process in place to estimate the impacts of proposed assumptions? (same as J29)

C22 Are the experience studies credible? Has company experience been compared to industry studies? If company experience is not fully credible, what other sources were used?

Has the experience study accounted for claims lag?

Does assumption development follow regulations?

Do assumptions reflect actual experience?

Have experience study results been used for assumption setting?

C27

If improvements in experience are projected, is the basis for improvement justified and understood? (e.g., mortality improvement, expense improvements.)

C28 Do the assumptions input into the model match the source documents? (same as D25)
C29 Are policy and asset input parameters correct and up to date (e.g., policy expenses,
crediting rates)?
C30 Are assumptions documented and approved (signed off) in accordance with a specified
process?
C31 Do people signing off on assumptions understand the results?

## Input Data/Tables/Mapping

Color Code: Governance Policy/Process-Type Questions

Validation-Type Questions

## General Guiding Questions

| D1 | Are model input feeds obtained automatically from either a centralized data warehouse |
| :--- | :--- |
| or another system? |  |
| D2 | Has the organization automated and standardized a set of test analytics for validation of <br> model input? <br> H3 <br> existing system)? If so, has the seriatim output been tested? |
| D4 | Has there been a new asset system implementation (or change to an existing system)? |
| D5 $\quad$ Are all data sources well defined and documented? |  |

## Questions of a More Technical Focus

D6 Are there checks in place to assure that policy data and inforce asset files were not unintentionally changed?

D7 Is model input data maintained and identifiable after model runs? Are there controls in place to ensure that input data cannot be modified prior to a model run in such a way that would bypass an audit trail?

D8 Are there controls in place to prevent other business units from changing data without proper documentation and communication?

D9 Are automated Extract Transform and Load (ETL) data processing steps tested when integrating data from source systems? (same as J28)

| D10 | Were the seriatim policy data correctly updated? |
| :---: | :---: |
| D11 | Do the policy data and inforce asset files contain the correct fields and filters? |
| D12 | Were the policy data and inforce data files selected for the right periods? |
| D13 | Were economic scenario files updated for the current start date? |
| D14 | Has policy or asset data been corrupted or partially loaded during the data extraction <br> process? |
| D15 | Have there been new policy data mappings? |
| D16 | Does the underlying policy data contain material errors? If so, are subsequent <br> adjustments to account for such errors properly documented? |
| D17 | Have there been administrative changes that affect the processing of policy or asset data <br> which could lead to incorrect results? |
| D18 | Have the model point files and mappings been updated correctly? |
| D19 | Do the model point files have correct file formats? |
| D20 | Are policies grouped correctly for the modeling purpose? Similarly, are assets grouped <br> correctly for the modeling purpose? |
| D21 | Were policy data groupings executed without loss of data or data corruption? Similarly, <br> were asset data groupings executed without loss of data or data corruption? |
| D22 | Are grouping rules appropriate, reflecting changes in the underlying policy data? <br> Similarly, are grouping rules appropriate, reflecting changes in the underlying asset data? |
| D23 | Have existing assets been reconciled to investment reporting systems? |
| D24 | Is the input data consistent with the data that was approved and signed off? |
| D25 | Do the assumptions input into the model match the source documents? (same as C28) |
| D26 | Are model input parameters correct and up to date (e.g., time horizon, valuation date)? |
| D27 | Were the correct input tables picked up in the model? |
| D28 | Have inputs to the results consolidation process been updated correctly? |

## General Guiding Questions

E1 Are access controls in place for models and modeling systems?

E2 Is there a documented process for granting access to models, systems, assumptions, reports, etc.?

E3 If modeling assumptions, results, or other components are archived in a centralized database/repository, is write access to this repository controlled?

E4 Are access levels reviewed on a regular basis?

## Questions of a More Technical Focus

E5 Is write access to model input databases controlled and limited?

E6 Is write access to model output databases controlled and limited?

F System / Model Changes

Color Code: Governance Policy/Process-Type Questions

Validation-Type Questions

## General Guiding Questions

F1 For homegrown or open-code modeling systems, are there system development and design standards in place for coding changes?

F2 Are coding changes peer reviewed?

F3 Is there a validation process in place to assure that the correct model and system versions have been released into production environments?

F4 Is there clear separation between development and production environments? Are appropriate development processes executed off-cycle (e.g., scheduling some of the work before the monthly/quarterly reporting cycle)?

F5 Are model and system changes achieved through a formal change management process?

F6 Is there a documented approval process in place for model and system changes?

| F7 | Are there documented coding/development guidelines in place for system changes? |
| :---: | :---: |
| F8 | In the context of vendor software, system upgrades are separate from logic/calculation <br> changes. Have protocols been established for system upgrades (including regression <br> testing)? |
| F9 | Has the organization implemented an automated attribution testing process to identify <br> and quantify differences due to system, logic, or data updates? (This is especially <br> important when using a common model of record.) |
| F10 | Is there a controlled, documented process for "checking out" production models (e.g., for <br> model development, sensitivity testing, new pricing, other analysis)? |
| F11 | Is the organization managing to a calendar for internal model releases to ensure <br> consistency of the model of record across the organization? |
| F12 | Is there clear accountability for code changes, bug fixes, improvements, etc.? |
| F13 | Are code comparison software tools in use? |
| F14 | Are there dedicated model/system/data stewards? |
| F15 | Are test beds (test packs) in use? (same as $\mathbf{J} \mathbf{1 7}$ ) |
| F16 | Do new system or model changes that are placed in production have corresponding <br> documentation describing the changes compared to the prior production version? |
| F17 | Is there a published schedule of new and past system/model version releases? |
| F18 | Are models and systems appropriately archived, along with associated inputs, to enable <br> the regeneration of results? |
| F19 | Is there a process to assure that protocols and procedures have been followed prior to <br> promoting system and model changes? |
| F20 | Do peer reviewers and testers report through a separate management reporting chain <br> than the modelers making the model/system changes? |
| F21 | Are modeling and system procedures reviewed on a regular basis? |
| F22 | Are test bed results archived for new system/model releases? |
| F23 | Is there a documented testing protocol for model and system changes? |
| F24 | Are there automated regression tests to assure that model/system changes do not <br> introduce unintended effects? (same as $\mathbf{J} \mathbf{2 5}$ ) |
| F25 | Are all the modeling components versioned (i.e., the code, reports, test cases, etc.)? |
| F26 | Is user acceptance testing (UAT) routinely performed and documented when there are <br> modeling system changes? |
| F27 | Are test packs (or test beds) kept up to date with new models, product features, and asset <br> types? (same as $\mathbf{J 2 2}$ ) |
| F28 | Are modeling systems well documented? |


| F29 | Are system changes well documented? |
| :--- | :--- |
| F30 | Are there helpful, descriptive comments in the code? |
| F31 | Are test results for new system/model production versions well documented and <br> accessible to modelers? |
| F32 | Are internal stakeholders' (i.e., those within the organization who rely on the results <br> either directly or indirectly) approvals obtained and documented prior to moving model/ <br> system changes into production? |

## Questions of a More Technical Focus

F33 Are requested functionality/feature modification requirements well documented, unambiguous, and correctly stated?

F34

F35

F36

F37

F38

F39

F40

F41

F42

F43

F44

Do proposed model designs correctly accommodate the requested change?

Does the proposed system/model design adhere to development standards?

Were the modeling system designs developed correctly?

Was the model independently baseline tested?

Were all product features and asset types modeled? Is the justification for those not modeled documented?

Was model coverage under baseline testing adequate for the system modification?

Are system/model errors adequately documented?

Have system coding changes been adequately tested?

Has a well-documented analysis of change process been consistently applied across each system/model change?

Are exceptions to the system/modeling protocols well documented with respect to:

- Reasons for the exception?
- Associated testing and test results?
- Exception approvals?

When exceptions occur, are changes to the protocol being considered to reduce the need for future exceptions?

G

# Model Selection / Versioning 

Color Code: $\quad$ Governance Policy/Process-Type Questions

Validation-Type Questions

## General Guiding Questions

G1 Is there a validation process in place to assure that the correct model and system versions have been released into production environments?

G2 Are reports automatically labeled to indicate data and version sources?

G3 Have experienced modeling experts been involved in algorithm selection and implementation of modeling concepts?

G4 Are all the modeling components versioned (i.e., the code, reports, test cases, etc.)?

## Questions of a More Technical Focus

G5 Was the correct model and version used?

G6 Is the input data consistent with the data that was aproved and signed off?

G7 Were the correct input tables picked up in the model?

G8 Has the correct results consolidation process version been used?

G9 Was the "purpose" of the model documented?

## General Guiding Questions

H1 Is the reporting process documented? Is there specific documentation describing how reporting levels are mapped to one another? (same as I1)

H2 Is model output stored in a data warehouse that can be queried? This allows for additional analysis and evaluation of model results.

H3 Have all products been accounted for in consolidated results?

H4 Have consolidated results been adjusted to account for unmodeled business?

## Questions of a More Technical Focus

H5 Is the results-consolidation process well designed?

H6 Has the organization standardized the model output that is used for reporting and analysis?

H7 Is write-access to model output databases controlled/limited?

H8 Do model outputs automatically contain documented references to the executed model input data and the model version used?

H9 Have the correct model results been retrieved?

H10 Are modeling results correctly interpreted within the consolidation process? (e.g., is it clear when reserves are net of reinsurance?) (similar to $\mathbf{1 2 4}$ and $\mathbf{J 3 5}$ )

H11 Has the correct results consolidation process version been used?

H12 Have inputs to results consolidation process been updated correctly?

H13 Are late adjustments to consolidated results implemented across all related fields? (e.g., when adjustments are made to account for unmodeled premiums, are they also made to account for the unmodeled reserves?) (same as J36)

H14 Are late adjustments consistently applied? (e.g., are adjustments in one reporting basis applied, as appropriate, to all other reporting bases?) (same as J37)

H15 Have summarized results been aggregated correctly from the lowest modeled level through each step up to the highest rollup consolidation?

## Reporting

Color Code:

Governance Policy/Process-Type Questions

Validation-Type Questions

## General Guiding Questions

I1

12

13

14

15

16

17

18

19

I10

I11

I12

I13

I14

I15

I16

117

118
Is the reporting process documented? Is there specific documentation describing how reporting levels are mapped to one another? (same as $\mathbf{H 1}$ )

Are staff trained on the reporting process, and is the reporting process reviewed on a regular basis by senior staff to ensure all significant items are considered?

How is the analysis of results documented? Is commentary captured and preserved alongside the results, as evidence that key results are understood and explained?

Are automated data feeds used to populate reports, as opposed to manual report preparation?

Are reports automatically labeled to indicate data and version source?

Are reports regularly reviewed by an experienced actuary or other subject matter expert?

To ensure the structure/content are consistent with regulatory requirements, are reports periodically reviewed "independently"? (Potential independent reviewers could include: compliance, internal or external auditors, or consultants.)

The larger the organization, the greater the risk. For large organizations, are the reviewers trained and educated well in advance of the model results/reporting review meeting? This ensures that reviewers have the required level of understanding to be able to perform a meaningful review.

Are documented procedures in place for identifying and making adjustments to model output? For example, a financial projection may use approved budgets for expense assumptions while pricing might use a different basis or set of expenses.

Are documented procedures in place to adjust model output for excluded products and/or assets? For example, are model results grossed up to reflect $100 \%$ of all inforce products?

Is the report labeled to indicate the intended audience?

Do reports include appropriate caveats to ensure they are not used for unintended purposes?

Is model output stored in a data warehouse that can be queried? This allows for additional analysis and evaluation of model results.

Do reports clearly state which model and data versions were used?

Have all products been accounted for in consolidated results?

Did the process owner and business owner sign off on the report?

Are all the modeling components versioned (i.e., the code, reports, test cases, etc.)?

Do the frequency and timing of reports align with the decisions which they support?

## Questions of a More Technical Focus

I19 Has the organization standardized the model output that is used for reporting and analysis?

120 Do model outputs automatically contain documented references to the executed model input data and the model version used?

I21 Are business users made aware of parameters that fall outside of agreed ranges or present other irregularities?

122 Do the reports indicate how robust or sensitive key figures are, along with impacts due to estimation error of input parameters?

I23 Have the correct model results been retrieved?

I24 Are modeling results well understood within the reports? (e.g., is it clear when reserves are net of reinsurance?) (similar to $\mathbf{H} \mathbf{1 0}$ and $\mathbf{J} \mathbf{3 5}$ )

I25 Have summarized results been aggregated correctly from the lowest modeled level through each step up to the highest rollup consolidation?

I26 Are results communicated using institutionally accepted metrics that are commonly understood by the reporting audience?

## Analysis / Validation

Color Code: $\square$ Governance Policy/Process-Type Questions

Validation-Type Questions

## General Guiding Questions

J1 Is there a validation process in place to ensure that the correct model and system versions have been released into production environments?

J2 Is there a validation process in place to ensure the model is producing reasonable results?

J3 Is there a prescribed set of sensitivity tests performed for each modeling exercise? How are the sensitivity tests defined and approved?

J4 Is there a prescribed set of stress tests performed for each modeling exercise? How are the stress tests defined and approved?

J5 Are assumptions (including both deterministic and stochastic) given an independent review?

J6

When setting assumptions, is the "level of analysis" reviewed by a senior staff member to ensure such "level of analysis" is neither too broad nor too narrow for the assessment and intended application of the assumption? (same as C3)

| J7 | Are automated checks/reconciliations built into the analysis of results process? |
| :---: | :---: |
| J8 | Has an appropriate amount of senior review time been built into the close schedule? |
| J9 | Is there clear separation between development and production environments? Are <br> appropriate development processes executed off-cycle (e.g., scheduling some of the work <br> before the monthly/quarterly reporting cycle)? |
| J10 | Has the organization planned and budgeted for the time and cost of analysis of results? |
| J11 | Are staff trained on the reporting process, and is the reporting process reviewed on a <br> regular basis by senior staff to ensure all significant items are considered? |
| J12 | How is the analysis of results documented? Is commentary captured and preserved <br> alongside the results, as evidence that key results are understood and explained? |
| J13 | Has the organization implemented an automated attribution testing process to identify <br> and quantify differences due to system, logic, or data updates? (This is especially <br> important when using a common model of record.) |
| J14 | Prior to changing an assumption, is an expectation formed and documented of the <br> impact of the change? (Model results can then be compared to these expectations.) |
| J15 | Has the organization automated and standardized a set of test analytics for validation of <br> model input? |
| J16 | Is model output stored in a data warehouse that can be queried? This allows for <br> additional analysis and evaluation of model results. |
| J17 | Are test beds (test packs) in use? (same as F15) |
| J18 | Are test bed results archived for new system/model releases? |
| J19 | Are all the modeling components versioned (i.e., the code, reports, test cases, etc.)? |
| J20 | Are test coverage sample sets (or reports) available identifying the products, product <br> features, and combinations that have been tested? |
| J21 | Is user acceptance testing (UAT) routinely performed and documented when there are <br> modeling system changes? |
| J22 | Are test packs (or test beds) kept up to date with new models, product features, and asset <br> types? (same as F27) |
| J23 | Is there documented evidence of a formal change management process? |

## Questions of a More Technical Focus

J24 Are the impacts of dynamic assumptions tested under different economic scenarios (e.g., dynamic lapse assumptions)? (same as C20)

J25 Are there automated regression tests to assure that model/system changes do not introduce unintended effects? (same as F24)

J26 Are test cases independently specified by business domain experts?

| J27 | Are negative test cases performed using inadmissible parameters that should result in <br> either aborted calculations or error indicators? |
| :---: | :---: |
| J28 | Are automated Extract Transform and Load (ETL) data processing steps tested when <br> integrating data from source systems?(same as D9) |
| J29 | Is there a process in place to estimate the impacts of proposed assumptions? <br> (same as $\mathbf{C 2 1}$ ) |
| J30 | Were the modeling system designs developed correctly? |
| J31 | Was the model independently baseline tested? |
| J32 | Have system coding changes been adequately tested? |
| J33 | Are the experience studies credible? |
| J34 | Has care and consideration been adequately applied to ensure that modeling system <br> capacities have not been exceeded, where overflow results may have been dropped? |
| J35 | Are modeling results correctly interpreted within the consolidation and reporting <br> processes? (e.g., is it clear when reserves are net of reinsurance?) (similar to $\mathbf{H 1 0}$ and $\mathbf{I 2 4}$ ) |
| J36 | Are late adjustments to consolidated results implemented across all related fields? (e.g., <br> when adjustments are made to account for unmodeled premiums, are they also made to <br> account for the unmodeled reserves?) (same as $\mathbf{H 1 3}$ ) |
| J37 | Are late adjustments consistently applied? (e.g., are adjustments in one reporting basis <br> applied, as appropriate, to all other reporting bases?) (same as H14) |
| J38 | Have summarized results been aggregated correctly from the lowest modeled level <br> through each step up to the highest rollup consolidation? |
| J39 | Has a well-documented analysis of change process been consistently applied across each <br> system / model change? |
| J40 | Have appropriate methods been performed and documented in the validation of model <br> fit (e.g., back-testing)? |
| J41 | Has an analysis of change, from a validated model to a modified model, based on an <br> automated attribution testing process been performed and documented? |

<img src="https://cdn.mathpix.com/cropped/2024_03_24_c6ed40200ab25ed70b4dg-20.jpg?height=135&width=312&top_left_y=1212&top_left_x=820" alt="image" style="width:100%;height:auto;">

AMERICAN ACADEmY of ACTUARIES

Objective. Independent. Effective. ${ }^{\text {Ð¼ }}$

1850 M Street NW, Suite 300

Washington, D.C. 20036

202-223-8196

ACTUARY.ORG

The American Academy of Actuaries is an 18,500 + member professional association whose mission is to serve the public and the U.S. actuarial profession. For more than 50 years, the Academy has assisted public policymakers on all levels by providing leadership, objective expertise, and actuarial advice on risk and financial security issues. The Academy also sets qualification, practice, and professionalism standards for actuaries in the United States.

